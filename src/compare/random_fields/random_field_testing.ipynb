{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from spicy_snow.retrieval import retrieval_from_parameters\n",
    "\n",
    "# import functions for downloading\n",
    "from spicy_snow.download.sentinel1 import s1_img_search, hyp3_pipeline, download_hyp3, combine_s1_images\n",
    "from spicy_snow.download.forest_cover import download_fcf\n",
    "from spicy_snow.download.snow_cover import download_snow_cover\n",
    "\n",
    "# import functions for pre-processing\n",
    "from spicy_snow.processing.s1_preprocessing import merge_partial_s1_images, s1_orbit_averaging,\\\n",
    "s1_clip_outliers, subset_s1_images, ims_water_mask, s1_incidence_angle_masking, merge_s1_subsets\n",
    "\n",
    "# import the functions for snow_index calculation\n",
    "from spicy_snow.processing.snow_index import calc_delta_VV, calc_delta_cross_ratio, \\\n",
    "    calc_delta_gamma, clip_delta_gamma_outlier, calc_snow_index, calc_snow_index_to_snow_depth\n",
    "\n",
    "# import the functions for wet snow flag\n",
    "from spicy_snow.processing.wet_snow import id_newly_frozen_snow, id_newly_wet_snow, \\\n",
    "    id_wet_negative_si, flag_wet_snow\n",
    "\n",
    "# setup root logger\n",
    "from spicy_snow.utils.spicy_logging import setup_logging\n",
    "\n",
    "# fischer z test\n",
    "from causallearn.utils.cit import CIT\n",
    "\n",
    "# random fields\n",
    "from gstools import SRF\n",
    "from gstools import vario_estimate_unstructured\n",
    "from gstools import Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s1_to_sd(ds, A = 1.5, B = 0.1, C = 0.6):\n",
    "    # mask out outliers in incidence angle\n",
    "    ds = s1_incidence_angle_masking(ds)\n",
    "    \n",
    "    # subset dataset by flight_dir and platform\n",
    "    dict_ds = subset_s1_images(ds)\n",
    "\n",
    "    for subset_name, subset_ds in dict_ds.items():\n",
    "        # average each orbit to overall mean\n",
    "        dict_ds[subset_name] = s1_orbit_averaging(subset_ds)\n",
    "        # clip outlier values of backscatter to overall mean\n",
    "        dict_ds[subset_name] = s1_clip_outliers(subset_ds)\n",
    "    \n",
    "    # recombine subsets\n",
    "    ds = merge_s1_subsets(dict_ds)\n",
    "\n",
    "    # calculate confidence interval\n",
    "    # ds = add_confidence_angle(ds)\n",
    "\n",
    "    ## Snow Index Steps\n",
    "    # log.info(\"Calculating snow index\")\n",
    "    # calculate delta CR and delta VV\n",
    "    ds = calc_delta_cross_ratio(ds, A = A)\n",
    "    ds = calc_delta_VV(ds)\n",
    "\n",
    "    # calculate delta gamma with delta CR and delta VV with FCF\n",
    "    ds = calc_delta_gamma(ds, B = B)\n",
    "\n",
    "    # clip outliers of delta gamma\n",
    "    ds = clip_delta_gamma_outlier(ds)\n",
    "\n",
    "    # calculate snow_index from delta_gamma\n",
    "    ds = calc_snow_index(ds, ims_masking = True)\n",
    "\n",
    "    # convert snow index to snow depth\n",
    "    ds = calc_snow_index_to_snow_depth(ds, C = C)\n",
    "\n",
    "    return ds\n",
    "\n",
    "import warnings\n",
    "def get_gaussian_stats(da):\n",
    "    arr = da.values\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Degrees of freedom <= 0 \")\n",
    "    return arr.size, np.nanmean(arr), np.nanstd(arr), arr.shape\n",
    "\n",
    "def bias(x, y): return np.mean(x - y)\n",
    "\n",
    "def get_stats(x, y, nrmse = False):\n",
    "    if type(x) == xr.DataArray: x = x.values.ravel()\n",
    "    if type(y) == xr.DataArray: y = y.values.ravel()\n",
    "    if type(x) == list: x = np.array(x)\n",
    "    if type(y) == list: y = np.array(y)\n",
    "    idx = (~np.isnan(x)) & (~np.isnan(y))\n",
    "    x, y = x[idx], y[idx]\n",
    "    r, p = pearsonr(x, y)\n",
    "    b = bias(x, y)\n",
    "    mae = mean_absolute_error(x, y)\n",
    "    rmse = mean_squared_error(x, y, squared = False)\n",
    "\n",
    "    if nrmse:\n",
    "        nrmse_value = rmse / np.mean(x)\n",
    "        return r, b, mae, rmse, nrmse_value\n",
    "\n",
    "    return r, b, mae, rmse\n",
    "\n",
    "from scipy.stats import norm\n",
    "def fischerz(truth, x1, x2):\n",
    "    idx = (~np.isnan(truth)) & (~np.isnan(x1)) & (~np.isnan(x2))\n",
    "    truth, x1, x2 = truth[idx], x1[idx], x2[idx]\n",
    "    n = len(x1)\n",
    "    cor1 = pearsonr(truth, x1).statistic\n",
    "    cor2 = pearsonr(truth, x2).statistic\n",
    "    fischer1 = 0.5*np.log((1+cor1)/(1-cor1))\n",
    "    fischer2 = 0.5*np.log((1+cor2)/(1-cor2))\n",
    "    expected_sd = np.sqrt(1/(n-3))\n",
    "    return 2 * (1 - norm(0, expected_sd).cdf(np.abs(fischer1 - fischer2)))\n",
    "\n",
    "def optimize(r_ds, A, B, C, im_date):\n",
    "    # holds rmse and r for optimizing rand dataset\n",
    "    rand_rmse_ds = xr.DataArray(np.zeros([len(A), len(B), len(C)]), dims = ['A', 'B', 'C'], coords = [A, B, C])\n",
    "    rand_mae_ds = xr.DataArray(np.zeros([len(A), len(B), len(C)]), dims = ['A', 'B', 'C'], coords = [A, B, C])\n",
    "    rand_r_ds = xr.DataArray(np.zeros([len(A), len(B), len(C)]), dims = ['A', 'B', 'C'], coords = [A, B, C])\n",
    "\n",
    "    # optimize random dataset\n",
    "    for a in A:\n",
    "        for b in B:\n",
    "            for c in C:\n",
    "                r_ds = s1_to_sd(r_ds, A = a, B = b, C = c)\n",
    "                rand_r, rand_b, rand_mae, rand_rmse = get_stats(r_ds['snow_depth'].sel(time = im_date, method = 'nearest'), r_ds['lidar-sd'], nrmse = False)\n",
    "                rand_rmse_ds.loc[dict(A = a, B = b, C = C)] = rand_rmse\n",
    "                rand_mae_ds.loc[dict(A = a, B = b, C = C)] = rand_mae\n",
    "                rand_r_ds.loc[dict(A = a, B = b, C = C)] = rand_r\n",
    "    \n",
    "    return rand_rmse_ds, rand_mae_ds, rand_r_ds\n",
    "\n",
    "def create_unstructured_grid(x_s, y_s):\n",
    "    x_u, y_u = np.meshgrid(x_s, y_s)\n",
    "    len_unstruct = len(x_s) * len(y_s)\n",
    "    x_u = np.reshape(x_u, len_unstruct)\n",
    "    y_u = np.reshape(y_u, len_unstruct)\n",
    "    return x_u, y_u\n",
    "\n",
    "def create_random_field(da):\n",
    "    random_ds = da.copy(deep = True)\n",
    "    arr = da.data\n",
    "    x_u, y_u = create_unstructured_grid(da.x.data, da.y.data)\n",
    "    bins = np.linspace(20, 1500, 20)\n",
    "    bin_center, gamma = vario_estimate_unstructured(\n",
    "            (x_u, y_u), arr.flatten(), bins, sampling_size=3000)\n",
    "    fit_model = Exponential(dim=2, nugget= 0)\n",
    "    fit_model.fit_variogram(bin_center, gamma, nugget=True)\n",
    "    srf = SRF(fit_model, seed=19770928)\n",
    "    # this transpose appears in all the documentation and is neccessary to make the data fit the original data\n",
    "    random_ds.data = srf((da.x.data, da.y.data), mesh_type='structured').T\n",
    "\n",
    "    return random_ds\n",
    "\n",
    "def make_random_timeseries(ds):\n",
    "    \"\"\"\n",
    "    Makes a random time series of s1 data with t0 VV having the same spatial correlation length and variances\n",
    "    as the real data and VH @ t = 0 being a mean shifted version of the random VV @ t0. Then the difference from \n",
    "    t = 0 to t = n is calculated for eachtime step and a change with the same varaince and spatial correlation length\n",
    "    is added to backscatter to t = 0.\n",
    "    \"\"\"\n",
    "    ds = ds.rio.write_crs('EPSG:4326')\n",
    "    random_ds = ds[['s1','fcf', 'ims']].copy(deep = True)\n",
    "\n",
    "    for orbit_name, sub in ds.groupby('relative_orbit'):\n",
    "\n",
    "        for band in ['VV','VH']:\n",
    "            sub[band] = sub['s1'].sel(band = band)\n",
    "        sub = sub.drop('s1')\n",
    "        sub = sub.drop('band')\n",
    "        sub = sub.transpose('time', 'y', 'x')\n",
    "        sub = sub.rio.reproject(sub.rio.estimate_utm_crs())\n",
    "        \n",
    "        \n",
    "        random_sub_ds = sub.copy(deep = True)\n",
    "        \n",
    "        # make a random visualization of VV @ t = 0 with matching variance and correlation length to real data\n",
    "        random_sub_ds['VV'].loc[dict(time = sub.isel(time = 0).time)] = create_random_field(sub['VV'].isel(time = 0))\n",
    "\n",
    "        # make VH @ t = 0 match VV with a shifted mean (arbitrary since we just use change in backscatter) to simulate the geometric drive correlation between the two \n",
    "        random_sub_ds['VH'].loc[dict(time = sub.isel(time = 0).time)] =  random_sub_ds['VV'].loc[dict(time = sub.isel(time = 0).time)] - 1\n",
    "        \n",
    "        sub['deltaVH'] = sub['VH'].diff(dim = 'time')\n",
    "\n",
    "        for i, (ts, time_ds) in enumerate(sub.groupby('time')):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            # go through each time series with dVV and then dVH\n",
    "            # add random field with matching spatial statistics to dVV and dVH to original image to make new timeseries\n",
    "            for band in ['VV', 'VH']:\n",
    "                random_sub_ds[band].loc[dict(time = ts)] = random_sub_ds[band].isel(time = 0) + create_random_field(time_ds[band] - random_sub_ds[band].isel(time = 0))\n",
    "        \n",
    "        \n",
    "        for band in ['VV', 'VH']:\n",
    "            random_ds['s1'].loc[dict(time = ds.relative_orbit == orbit_name, band = band)] = random_sub_ds[band].rio.reproject_match(random_ds['s1'].loc[dict(time = ds.relative_orbit == orbit_name, band = band)])\n",
    "\n",
    "    return random_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = Path('~/scratch/spicy/SnowEx-Data/').expanduser().resolve()\n",
    "data_dir = Path('~/scratch/spicy/SnowEx-Data/').expanduser().resolve()\n",
    "out_dir = Path('/bsuhome/zacharykeskinen/spicy-analysis/results/synthetic_compare')\n",
    "dss = {fp.stem: xr.open_dataset(fp) for fp in in_dir.glob('*.nc')}\n",
    "\n",
    "# Create parameter space\n",
    "A = np.round(np.arange(1, 3.1, 0.5), 2)\n",
    "B = np.round(np.arange(0, 2.01, 0.25), 2)\n",
    "C = np.round(np.arange(0.25, 1.001, 0.25), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random field starting with matching VV and VH (with a contant offset) from the spatial statistics of VV @ t =0 and then iteratively add a random field calculated from that time steps dVV and dVH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mores 2020 random created\n",
      "Mores 2020 done\n",
      "Fraser 2021 random created\n",
      "Fraser 2021 done\n",
      "Dry Creek 2020 random created\n",
      "Dry Creek 2020 done\n"
     ]
    }
   ],
   "source": [
    "if out_dir.joinpath('synthetic_v1.csv').exists():\n",
    "    res = pd.read_csv(out_dir.joinpath('synthetic_v1.csv'), index_col=0)\n",
    "else:\n",
    "    res = pd.DataFrame()\n",
    "rands =[]\n",
    "s1 = []\n",
    "sd = []\n",
    "\n",
    "# Create parameter space\n",
    "A = np.round(np.arange(1, 3.1, 0.5), 2)\n",
    "B = np.round(np.arange(0, 2.01, 0.25), 2)\n",
    "C = np.round(np.arange(0.1, 1.001, 0.2), 2)\n",
    "\n",
    "for stem, full_ds in dss.items():\n",
    "    full_ds = full_ds.load()\n",
    "    site_name = stem.replace('_', ' ').replace('Frasier', 'Fraser').split('-')[0]\n",
    "    \n",
    "    if site_name in res.index:\n",
    "        pass\n",
    "    \n",
    "    # if stem != 'Little_Cottonwood_2021-03-18':\n",
    "    #     continue\n",
    "    # full_ds = full_ds.load().isel(time = slice(50, None))\n",
    "\n",
    "    if stem == 'Frasier_2020-02-11':\n",
    "        im_date = pd.to_datetime('2020-02-16')\n",
    "    else:\n",
    "        im_date = pd.to_datetime(full_ds.sel(time = full_ds.attrs['lidar-flight-time'], method = 'nearest').time.values.ravel()[0])\n",
    "    \n",
    "    random_ds = make_random_timeseries(full_ds)\n",
    "\n",
    "    # print values\n",
    "    print(site_name + ' random created')\n",
    "    \n",
    "    random_ds['lidar-sd'] = full_ds['lidar-sd']\n",
    "    rand_rmse_ds, rand_mae_ds, rand_r_ds = optimize(random_ds, A, B, C, im_date)\n",
    "\n",
    "    a_best = rand_r_ds.max(['B', 'C']).idxmax('A')\n",
    "    b_best = rand_r_ds.max(['C', 'A']).idxmax('B')\n",
    "    c_best = rand_mae_ds.sel(A = a_best, B = b_best).idxmin('C')\n",
    "    \n",
    "    r_ds = s1_to_sd(random_ds, A = a_best, B = b_best, C = c_best).sel(time = im_date, method = 'nearest')\n",
    "    ds = full_ds.sel(time = im_date, method = 'nearest')\n",
    "\n",
    "\n",
    "    r, b, mae, rmse = get_stats(ds['snow_depth'], ds['lidar-sd'], nrmse = False)\n",
    "    rand_r, rand_b, rand_mae, rand_rmse = get_stats(r_ds['snow_depth'], r_ds['lidar-sd'], nrmse = False)\n",
    "    fischer_z = fischerz(ds['lidar-sd'].data.ravel(), ds['snow_depth'].data.ravel(), r_ds['snow_depth'].data.ravel())\n",
    "\n",
    "    res.loc[site_name, 'random_r'] = rand_r\n",
    "    res.loc[site_name, 'random_rmse'] = rand_rmse\n",
    "    res.loc[site_name, 'fischerz'] = fischer_z\n",
    "    res.loc[site_name, 'real_r'] = r\n",
    "    res.loc[site_name, 'real_rmse'] = rmse\n",
    "    \n",
    "    rands.append(r_ds['snow_depth'].data.ravel())\n",
    "    s1.append(ds['snow_depth'].data.ravel())\n",
    "    sd.append(ds['lidar-sd'].data.ravel())\n",
    "\n",
    "    res.to_csv(out_dir.joinpath('synthetic_v1.csv'))\n",
    "    print(site_name + ' done')\n",
    "\n",
    "rands, sd, s1 = np.concatenate(rands), np.concatenate(sd), np.concatenate(s1)\n",
    "\n",
    "rand_r, rand_b, rand_mae, rand_rmse = get_stats(rands, sd)\n",
    "r, b, mae, rmse = get_stats(s1, sd)\n",
    "\n",
    "res.loc['All Sites', 'random_r'] = rand_r\n",
    "res.loc['All Sites', 'random_rmse'] = rand_rmse\n",
    "res.loc['All Sites', 'real_r'] = r\n",
    "res.loc['All Sites', 'real_rmse'] = rmse\n",
    "res.loc['All Sites', 'fischerz'] = fischerz(sd, s1, rands)\n",
    "res.to_csv(out_dir.joinpath('synthetic_v1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding increasing gaussian noise to s1 vv and vh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reordering VV and VH images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spicy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
